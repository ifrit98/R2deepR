% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glu.R
\name{layer_glu}
\alias{layer_glu}
\title{Gated Linear Unit}
\usage{
layer_glu(
  object,
  filters = 32,
  kernel_size = 3,
  kernel_initializer = "glorot_normal",
  kernel_regularizer = NULL,
  bias_initializer = "zeros",
  bias_regularizer = NULL,
  name = NULL,
  trainable = TRUE
)
}
\description{
Computes hidden layer h_l as:
  h_l(X) = (X ∗ W + b) ⊗ σ(X ∗ V + c)
}
\references{
Dauphin et al. 2017 https://arxiv.org/pdf/1612.08083.pdf
}
